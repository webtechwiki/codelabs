
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>在 Debian 12 上完全手动安装 kubernetes v1.32.2</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/claat-public/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14" ga4id=""></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  codelab-ga4id=""
                  id="Fully-Manual-Installation-of-Kubernetes-v1.32.2-on-Debian-12"
                  title="在 Debian 12 上完全手动安装 kubernetes v1.32.2"
                  environment="web"
                  feedback-link="https://github.com/webtechwiki/codelabs/issues">
    
      <google-codelab-step label="一、基础环境准备" duration="0">
        <h2 is-upgraded>1.1 kubernetes集群包含的组件</h2>
<p>Kubernetes 集群由 **Master 节点（控制平面）**和 **Node 节点（工作节点）**组成，各自包含以下核心组件：</p>
<h3 is-upgraded>1.1.1 Master节点组件</h3>
<ol type="1">
<li><strong>API Server（kube-apiserver）</strong><ul>
<li>集群的入口，提供 REST API，处理所有操作请求（如创建、更新、删除资源）。</li>
<li>负责与其他组件通信（如 <code>kubelet</code>、<code>kube-scheduler</code> 等）。</li>
</ul>
</li>
<li><strong>etcd</strong><ul>
<li>分布式键值存储数据库，保存集群的所有状态和配置数据（如 Pod、Service、Namespace 等）。</li>
<li>是 Kubernetes 的&#34;唯一真实数据源&#34;（Single Source of Truth）。</li>
</ul>
</li>
<li><strong>Scheduler（kube-scheduler）</strong><ul>
<li>监听未调度的 Pod，根据资源需求、节点负载等因素，将 Pod 分配到合适的 Node 上运行。</li>
</ul>
</li>
<li><strong>Controller Manager（kube-controller-manager）</strong><ul>
<li>运行一系列控制器，确保集群状态与期望一致。</li>
<li>核心控制器包括： <ul>
<li>Node Controller（监控节点状态）</li>
<li>Deployment Controller（管理副本数）</li>
<li>Service Controller（管理 Service 与 Endpoint）</li>
<li>其他控制器（如 ReplicaSet、Namespace 控制器等）。</li>
</ul>
</li>
</ul>
</li>
<li><strong>（可选）Cloud Controller Manager</strong><ul>
<li>当集群运行在公有云环境时，负责与云平台交互（如负载均衡、存储卷、节点管理）。</li>
<li>解耦 Kubernetes 与特定云厂商的代码。</li>
</ul>
</li>
</ol>
<h3 is-upgraded>1.1.2 Node 节点组件</h3>
<ol type="1">
<li><strong>kubelet</strong><ul>
<li>运行在每个 Node 上的&#34;节点代理&#34;，负责： <ul>
<li>与 Master 通信，接收 Pod 定义（通过 API Server）。</li>
<li>管理 Pod 生命周期（启动、停止、监控容器）。</li>
<li>上报节点状态（如资源使用、Pod 状态）到 Master。</li>
</ul>
</li>
</ul>
</li>
<li><strong>kube-proxy</strong><ul>
<li>维护节点上的网络规则，实现 Service 的抽象（如负载均衡、服务发现）。</li>
<li>通过 iptables/IPVS 或用户空间代理转发流量到 Pod。</li>
</ul>
</li>
<li><strong>容器运行时（Container Runtime）</strong><ul>
<li>负责运行容器的底层软件，如 Docker、containerd、CRI-O。</li>
<li>与 Kubernetes 通过 CRI（Container Runtime Interface）交互。</li>
</ul>
</li>
</ol>
<h2 is-upgraded>1.2 集群主机规划</h2>
<p>所有机器的CPU都是x86的64位架构，并且安装了<code>Debian GNU/Linux 12 (bookworm)</code>。各个主机配置如下</p>
<table>
<tr><td colspan="1" rowspan="1"><p>主机</p>
</td><td colspan="1" rowspan="1"><p>IP</p>
</td><td colspan="1" rowspan="1"><p>操作系统</p>
</td><td colspan="1" rowspan="1"><p>配置</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>k8s-101</p>
</td><td colspan="1" rowspan="1"><p>192.168.122.101</p>
</td><td colspan="1" rowspan="1"><p>Debian GNU/Linux 12 (bookworm)</p>
</td><td colspan="1" rowspan="1"><p>内存:4G + SSD硬盘:30G + CPU:2核</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>k8s-102</p>
</td><td colspan="1" rowspan="1"><p>192.168.122.102</p>
</td><td colspan="1" rowspan="1"><p>Debian GNU/Linux 12 (bookworm)</p>
</td><td colspan="1" rowspan="1"><p>内存:4G + SSD硬盘:30G + CPU:2核</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>k8s-103</p>
</td><td colspan="1" rowspan="1"><p>192.168.122.103</p>
</td><td colspan="1" rowspan="1"><p>Debian GNU/Linux 12 (bookworm)</p>
</td><td colspan="1" rowspan="1"><p>内存:4G + SSD硬盘:30G + CPU:2核</p>
</td></tr>
</table>
<p>这三台主机在集群中分别充当的角色如下：</p>
<ul>
<li><code>k8s-101</code>: etcd服务器、控制节点、Proxy的L4、L7代理。</li>
</ul>
<p>同时作为运维主机，一些额外的服务由该主机提供，如：签发证书、dns服务、Docker的私有仓库服务、k8s资源配置清单仓库服务、共享存储（NFS）服务等。不过这些额外服务在需要的时候再安装，现在只是这么规划</p>
<ul>
<li><code>k8s-102</code>: etcd服务器、控制节点、工作节点、Proxy的L4、L7代理</li>
<li><code>k8s-102</code>: etcd服务器、控制节点、工作节点</li>
</ul>
<p>以上是在资源有限的情况下做的高可用资源分配，其实就是把其群的各个组件合理分配到这3台主机上。如果你的服务器资源充足，应当将各个服务分别独立部署到更多主机上，这样更加合理。</p>
<h2 is-upgraded>1.3 设置hostsname</h2>
<p>在 <code>192.168.122.101</code> 执行以下命令</p>
<pre><code language="language-bash" class="language-bash">hostnamectl set-hostname k8s-101
cat &gt;&gt; /etc/hosts &lt;&lt;EOF
192.168.122.101 k8s-101
EOF
</code></pre>
<p>在 <code>192.168.122.102</code> 执行以下命令</p>
<pre><code language="language-bash" class="language-bash">hostnamectl set-hostname k8s-102
cat &gt;&gt; /etc/hosts &lt;&lt;EOF
192.168.122.102 k8s-102
EOF
</code></pre>
<p>在 <code>192.168.122.103</code> 执行以下命令</p>
<pre><code language="language-bash" class="language-bash">hostnamectl set-hostname k8s-103
cat &gt;&gt; /etc/hosts &lt;&lt;EOF
192.168.122.103 k8s-103
EOF
</code></pre>
<h2 is-upgraded>1.4 关闭交换分区</h2>
<p>ubernetes 默认不支持在启用交换分区的情况下运行，可以使用以下命令临时关闭交换分区</p>
<pre><code language="language-bash" class="language-bash">swapoff -a
</code></pre>
<p>如果你希望永久禁用交换分区，可以编辑 <code>/etc/fstab</code> 文件，注释掉或删除与交换分区相关的行。然后运行以下命令确保交换分区被禁用：</p>
<pre><code language="language-bash" class="language-bash">swapoff -a
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="二、安装containerd" duration="0">
        <p>containerd的下载网址为<a href="https://containerd.io/downloads/" target="_blank">https://containerd.io/downloads/</a>，在撰写文章时（2025.02.15）最新版本是<code>v2.0.2</code>，安装到三台机器作为容器运行时环境，分别执行以下操作</p>
<h2 is-upgraded>2.1 安装 containerd</h2>
<p>从 <a href="https://github.com/containerd/containerd/releases" target="_blank">https://github.com/containerd/containerd/releases</a> 下载 <code>containerd-<版本>-<操作系统>-<架构>.tar.gz</code> 存档，验证其 sha256sum，并将其解压到 <code>/usr/local</code> 目录下</p>
<pre><code language="language-shell" class="language-shell">tar Cxzvf /usr/local containerd-2.0.2-linux-amd64.tar.gz
mkdir -p /usr/local/lib/systemd/system/
cat &gt; /usr/local/lib/systemd/system/containerd.service &lt;&lt;EOF
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target local-fs.target dbus.service

[Service]
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd

Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity

# Comment TasksMax if your systemd version does not supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target
EOF
systemctl daemon-reload
systemctl enable --now containerd
</code></pre>
<h2 is-upgraded>2.2 安装 runc</h2>
<p>runc 是一个轻量级的容器运行时工具，负责根据 OCI（Open Container Initiative）规范创建和运行容器。containerd 依赖 runc 来实际启动和管理容器。</p>
<p>从 <a href="https://github.com/opencontainers/runc/releases" target="_blank">https://github.com/opencontainers/runc/releases</a> 下载 <code>runc.<架构></code> 二进制文件，验证其 <code>sha256sum</code>，并将其安装为 <code>/usr/local/sbin/runc</code>。</p>
<pre><code language="language-shell" class="language-shell">install -m 755 runc.amd64 /usr/local/sbin/runc
</code></pre>
<p>该二进制文件是静态构建的，应该适用于任何 Linux 发行版。</p>
<h2 is-upgraded>2.3 安装 CNI 插件</h2>
<p>CNI（Container Network Interface）插件用于配置容器的网络，包括分配 IP 地址、设置网络接口、配置路由等。通常需要安装，除非明确不需要网络功能。</p>
<p>从 <a href="https://github.com/containernetworking/plugins/releases" target="_blank">https://github.com/containernetworking/plugins/releases</a> 下载 <code>cni-plugins-<操作系统>-<架构>-<版本>.tgz</code> 存档，验证其 <code>sha256sum</code>，并将其解压到 <code>/opt/cni/bin</code> 目录下：</p>
<pre><code language="language-shell" class="language-shell">mkdir -p /opt/cni/bin
tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.6.2.tgz
</code></pre>
<p>将 <code>/opt/cni/bin</code> 目录添加到 <code>$PATH</code> 中，执行以下命令追加到 <code>/etc/profile</code> 文件中</p>
<pre><code language="language-shell" class="language-shell">echo &#39;export PATH=$PATH:/opt/cni/bin&#39; &gt;&gt; /etc/profile
source /etc/profile
</code></pre>
<p>创建 CNI 配置文件目录</p>
<pre><code language="language-shell" class="language-shell">mkdir -p /etc/cni/net.d
cat &gt; /etc/cni/net.d/10-mynet.conf &lt;&lt;EOF
{
  &#34;cniVersion&#34;: &#34;0.4.0&#34;,
  &#34;name&#34;: &#34;mynet&#34;,
  &#34;type&#34;: &#34;bridge&#34;,
  &#34;bridge&#34;: &#34;cni0&#34;,
  &#34;isGateway&#34;: true,
  &#34;ipMasq&#34;: true,
  &#34;ipam&#34;: {
    &#34;type&#34;: &#34;host-local&#34;,
    &#34;subnet&#34;: &#34;10.22.0.0/16&#34;,
    &#34;routes&#34;: [
      { &#34;dst&#34;: &#34;0.0.0.0/0&#34; }
    ]
  }
}
EOF
</code></pre>
<p>重启 containerd</p>
<pre><code language="language-shell" class="language-shell">systemctl restart containerd
</code></pre>
<p>这些二进制文件是静态构建的，应该适用于任何 Linux 发行版。</p>
<h2 is-upgraded>2.4 使用命令行工具</h2>
<p><code>containerd</code> 是一个强大的容器运行时，但它本身是一个守护进程，需要通过命令行工具（CLI）来交互。不同的 CLI 工具（如 <code>ctr</code>、<code>nerdctl</code>、<code>crictl</code>）是为了满足不同用户和场景的需求而设计的。以下是它们的区别和适用场景：</p>
<table>
<tr><td colspan="1" rowspan="1"><p>工具</p>
</td><td colspan="1" rowspan="1"><p>目标用户</p>
</td><td colspan="1" rowspan="1"><p>功能特点</p>
</td><td colspan="1" rowspan="1"><p>适用场景</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><code>ctr</code></p>
</td><td colspan="1" rowspan="1"><p><code>containerd</code> 开发者或高级用户</p>
</td><td colspan="1" rowspan="1"><p><code>containerd</code> 自带的官方命令行工具，底层、简单、直接与 <code>containerd</code> 交互</p>
</td><td colspan="1" rowspan="1"><p>开发和调试 <code>containerd</code></p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><code>nerdctl</code></p>
</td><td colspan="1" rowspan="1"><p>普通用户和运维人员</p>
</td><td colspan="1" rowspan="1"><p>类似 Docker 的体验，功能丰富</p>
</td><td colspan="1" rowspan="1"><p>日常容器管理、生产环境</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><code>crictl</code></p>
</td><td colspan="1" rowspan="1"><p>Kubernetes 管理员和开发者</p>
</td><td colspan="1" rowspan="1"><p>针对 CRI 设计，适合 Kubernetes 环境</p>
</td><td colspan="1" rowspan="1"><p>调试 Kubernetes 节点和容器运行时</p>
</td></tr>
</table>
<p>在这里，我们额外安装 <code>nerdctl</code> 工具，以方便后续操作。在 <a href="https://github.com/containerd/nerdctl/releases" target="_blank">https://github.com/containerd/nerdctl/releases</a> 下载对应的操作系统版本，在撰写这边文章时 <code>nerdctl</code> 的版本是 <code>v2.0.3</code>，安装命令如下</p>
<pre><code language="language-shell" class="language-shell">wget https://github.com/containerd/nerdctl/releases/download/v2.0.3/nerdctl-2.0.3-linux-amd64.tar.gz
tar -zxvf nerdctl-2.0.3-linux-amd64.tar.gz -C /usr/bin/ nerdctl
</code></pre>
<p>最后，加载 <code>nerdctl</code> 的 <code>Bash</code> 自动补全功能，并设置 <code>containerd</code> 默认的名称空间为 <code>k8s.io</code>，如下</p>
<pre><code language="language-bash" class="language-bash"># 追加配置
cat &gt;&gt; /etc/profile &lt;&lt;EOF
source &lt;(nerdctl completion bash)
export CONTAINERD_NAMESPACE=k8s.io
EOF

# 让配置立即生效
source /etc/profile
</code></pre>
<h2 is-upgraded>2.5 配置 containerd</h2>
<p>containerd默认配置文件在 <code>/etc/containerd/config.toml</code>，通过运行以下命令生成一个默认配置文件：</p>
<pre><code language="language-shell" class="language-shell">mkdir -p /etc/containerd
containerd config default &gt; /etc/containerd/config.toml
</code></pre>
<p>重启 containerd</p>
<pre><code language="language-shell" class="language-shell">systemctl restart containerd
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="三、签发SSL证书" duration="0">
        <h2 is-upgraded>3.1 安装证书工具</h2>
<p><code>cfssl</code> 系列工具是 Cloudflare 提供的 PKI/TLS 工具，用于证书管理。可以在 <a href="https://github.com/cloudflare/cfssl" target="_blank">https://github.com/cloudflare/cfssl</a> 找到对应的信息，在撰写文章时版本是 <code>1.6.6</code>，我们下载对应操作系统的版本，安装到 <code>k8s-101</code> 这台主机，以 linux amd64 为例安装命令如下</p>
<pre><code language="language-shell" class="language-shell">wget https://github.com/cloudflare/cfssl/releases/download/v1.6.5/cfssl_1.6.5_linux_amd64 -o /usr/local/bin/cfssl
wget https://github.com/cloudflare/cfssl/releases/download/v1.6.5/cfssljson_1.6.5_linux_amd64 -o /usr/local/bin/cfssljson
wget https://github.com/cloudflare/cfssl/releases/download/v1.6.5/cfssl-certinfo_1.6.5_linux_amd64 -o /usr/local/bin/cfssl-certinfo
chmod a+x /usr/local/bin/cfssl*
</code></pre>
<p>以下是它们的简要功能：</p>
<ul>
<li><strong>cfssl</strong>：核心工具，用于证书生成和管理。</li>
<li><strong>cfssl-json</strong>：辅助工具，用于解析 JSON 输出。</li>
<li><strong>cfssl-certinfo</strong>：用于查看证书详细信息。</li>
</ul>
<h2 is-upgraded>3.2 k8s所需证书概述</h2>
<p>在Kubernetes集群中，我们需要为集群中的各个组件生成证书，以实现安全通信和身份验证。下图展示了Kubernetes所需的主要证书</p>
<p class="image-container"><img alt="k8s证书" src="img/e5518789521b4ac5.png"></p>
<p>我们将在 <code>k8s-101</code> 生成的各个证书存放到 <code>/etc/kubernetes/pki</code> 里，并同步到其他主机上。</p>
<h2 is-upgraded>3.3 搭建CA</h2>
<p>CA是证书的签发机构，签发证书的前提是有一个签发机构，下文我们搭建自己的签发机构。</p>
<p>使用以下命令生成CA配置</p>
<pre><code language="language-shell" class="language-shell">mkdir -p /etc/kubernetes/pki
cat &gt; /etc/kubernetes/pki/ca-config.json &lt;&lt;EOF
{
    &#34;signing&#34;: {
        &#34;default&#34;: {
            &#34;expiry&#34;: &#34;175200h&#34;
        },
        &#34;profiles&#34;: {
            &#34;www&#34;: {
                &#34;expiry&#34;: &#34;175200h&#34;,
                &#34;usages&#34;: [
                    &#34;signing&#34;,
                    &#34;key encipherment&#34;,
                    &#34;server auth&#34;,
                    &#34;client auth&#34;
                ]
            }
        }
    }
}
EOF
</code></pre>
<p>使用以下命令生成 CA 请求文件</p>
<pre><code language="language-shell" class="language-shell">cat &gt; /etc/kubernetes/pki/ca-csr.json &lt;&lt;EOF
{
    &#34;CN&#34;: &#34;kubernetes&#34;,
    &#34;key&#34;: {
        &#34;algo&#34;: &#34;rsa&#34;,
        &#34;size&#34;: 2048
    },
    &#34;names&#34;: [
        {
            &#34;C&#34;: &#34;CN&#34;,
            &#34;L&#34;: &#34;Guangzhou&#34;,
            &#34;ST&#34;: &#34;Guangdong&#34;,
            &#34;O&#34;: &#34;kubernetes&#34;,
            &#34;OU&#34;: &#34;system&#34;
        }
    ],
    &#34;ca&#34;: {
        &#34;expiry&#34;: &#34;175200h&#34;
    }
}
EOF
</code></pre>
<p>证书根字段</p>
<ul>
<li><code>CN</code>：证书名称</li>
<li><code>key</code>：定义证书类型，algo为加密类型，size为加密长度</li>
</ul>
<p><code>names</code> 定义证书的通用名称，可以有多个条目</p>
<ul>
<li><code>CN</code>: Common Name，一般使用域名</li>
<li><code>C</code>: Country Code，申请单位所属国家，只能是两个字母的国家码。例如，中国只能是CN。</li>
<li><code>ST</code>: State or Province，省份名称或自治区名称</li>
<li><code>L</code>: Locality，城市或自治州名</li>
<li><code>O</code>: Organization name，组织名称、公司名称</li>
<li><code>OU</code>: Organization Unit Name，组织单位名称、公司部门</li>
</ul>
<p><code>ca.expiry</code> 代表有效时间，175200h代表20年。</p>
<p>最后使用以下命令生成CA自签名根证书</p>
<pre><code language="language-shell" class="language-shell">cfssl gencert -initca ca-csr.json | cfssljson -bare ca
</code></pre>
<p>最后一个参数指定了证书文件名，最后生成以下三个文件</p>
<ul>
<li><code>ca.csr</code>: 证书签名申请（Certificate Signing Request）文件</li>
<li><code>ca.pem</code>: ca公钥证书</li>
<li><code>ca-key.pem</code>: ca私钥证书</li>
</ul>
<p>生成的三个文件是根证书包含的内容。后续，我们给各个服务颁发证书的时候，都基于CA根证书来颁发。</p>
<h2 is-upgraded>3.4 签发证书</h2>
<ul>
<li>etcd</li>
</ul>
<p>定义证书信息如下</p>
<pre><code language="language-shell" class="language-shell">cat &gt; /etc/kubernetes/pki/etcd-csr.json &lt;&lt;EOF
{
    &#34;CN&#34;: &#34;etcd&#34;,
    &#34;hosts&#34;: [
        &#34;127.0.0.1&#34;,
        &#34;192.168.122.101&#34;,
        &#34;192.168.122.102&#34;,
        &#34;192.168.122.103&#34;
    ],
    &#34;key&#34;: {
        &#34;algo&#34;: &#34;rsa&#34;,
        &#34;size&#34;: 2048
    },
    &#34;names&#34;: [{
        &#34;C&#34;: &#34;CN&#34;,
        &#34;ST&#34;: &#34;Guangdong&#34;,
        &#34;L&#34;: &#34;Guangzhou&#34;,
        &#34;O&#34;: &#34;kubernetes&#34;,
        &#34;OU&#34;: &#34;system&#34;
    }]
}
EOF
</code></pre>
<p>支持的主机列表对应本机以及所有 etcd 节点。使用以下命令生成证书</p>
<pre><code language="language-shell" class="language-shell">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www etcd-csr.json | cfssljson -bare etcd
</code></pre>
<ul>
<li>kube-apiserver</li>
</ul>
<p>k8s的其他组件跟 apiserver 要进行双向TLS（mTLS）认证，所以 apiserver 需要有自己的证书，以下定义证书申请文件</p>
<pre><code language="language-shell" class="language-shell">cat &gt; /etc/kubernetes/pki/apiserver-csr.json &lt;&lt;EOF
{
    &#34;CN&#34;: &#34;apiserver&#34;,
    &#34;key&#34;: {
        &#34;algo&#34;: &#34;rsa&#34;,
        &#34;size&#34;: 2048
    },
    &#34;hosts&#34;: [
        &#34;127.0.0.1&#34;,
        &#34;10.96.0.1&#34;,
        &#34;192.168.122.100&#34;,
        &#34;192.168.122.101&#34;,
        &#34;192.168.122.102&#34;,
        &#34;192.168.122.103&#34;,
        &#34;kubernetes&#34;,
        &#34;kubernetes.default&#34;,
        &#34;kubernetes.default.svc&#34;,
        &#34;kubernetes.default.svc.cluster&#34;,
        &#34;kubernetes.default.svc.cluster.local&#34;
    ],
    &#34;names&#34;: [{
        &#34;C&#34;: &#34;CN&#34;,
        &#34;ST&#34;: &#34;Guangdong&#34;,
        &#34;L&#34;: &#34;Guangzhou&#34;,
        &#34;O&#34;: &#34;kubernetes&#34;,
        &#34;OU&#34;: &#34;system&#34;
    }]
}
EOF
</code></pre>
<p>该证书后续被 kubernetes master 集群使用，需要将 master 节点的 IP 都填上，同时还需要填写 service 网络的第一个IP（后续计划使用<code>10.96.0.0 255.255.0.0</code> 网段作为service网络，因此加上 <code>10.96.0.1</code>），后续可能加到集群里的IP也需要都填写上去。最后使用以下命令生成证书</p>
<pre><code language="language-shell" class="language-shell">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www apiserver-csr.json | cfssljson -bare apiserver
</code></pre>
<p>– kube-controller-manager</p>
<p>controller-manager需要跟apiserver进行mTLS认证，定义证书申请文件如下</p>
<pre><code language="language-shell" class="language-shell">cat &gt; /etc/kubernetes/pki/controller-manager-csr.json &lt;&lt;EOF
{
  &#34;CN&#34;: &#34;system:kube-controller-manager&#34;,
  &#34;key&#34;: {
    &#34;algo&#34;: &#34;rsa&#34;,
    &#34;size&#34;: 2048
  },
   &#34;hosts&#34;: [
     &#34;127.0.0.1&#34;,
     &#34;192.168.122.100&#34;,
     &#34;192.168.122.101&#34;,
     &#34;192.168.122.102&#34;,
     &#34;192.168.122.103&#34;
    ],
  &#34;names&#34;: [
    {
      &#34;C&#34;: &#34;CN&#34;,
      &#34;ST&#34;: &#34;Guangdong&#34;,
      &#34;L&#34;: &#34;Guangzhou&#34;,
      &#34;O&#34;: &#34;system:kube-controller-manager&#34;,
      &#34;OU&#34;: &#34;system&#34;
    }
  ]
}
EOF
</code></pre>
<p>hosts 列表包含所有 kube-controller-manager 节点 IP；CN 为 system:kube-controller-manager，O 为 system:kube-controller-manager，k8s里内置的ClusterRoleBindings system:kube-controller-manager 授权 kube-controller-manager所需的权限。后面组件证书都做类似操作。生成证书命令如下</p>
<pre><code language="language-shell" class="language-shell">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www controller-manager-csr.json | cfssljson -bare controller-manager
</code></pre>
<ul>
<li>kube-scheduler</li>
</ul>
<p>kube-scheduler需要跟apiserver进行mTLS认证，生成证书申请文件如下</p>
<pre><code language="language-shell" class="language-shell">cat &gt; /etc/kubernetes/pki/scheduler-csr.json &lt;&lt;EOF
{
  &#34;CN&#34;: &#34;system:kube-scheduler&#34;,
  &#34;key&#34;: {
    &#34;algo&#34;: &#34;rsa&#34;,
    &#34;size&#34;: 2048
  },
   &#34;hosts&#34;: [
     &#34;127.0.0.1&#34;,
     &#34;192.168.122.100&#34;,
     &#34;192.168.122.101&#34;,
     &#34;192.168.122.102&#34;,
     &#34;192.168.122.101&#34;
    ],
  &#34;names&#34;: [
    {
      &#34;C&#34;: &#34;CN&#34;,
      &#34;ST&#34;: &#34;Guangdong&#34;,
      &#34;L&#34;: &#34;Guangzhou&#34;,
      &#34;O&#34;: &#34;system:kube-scheduler&#34;,
      &#34;OU&#34;: &#34;system&#34;
    }
  ]
}
EOF
</code></pre>
<p>kubernetes内置的ClusterRoleBindings system:kube-scheduler将授权kube-scheduler所需的权限。生成证书命令如下</p>
<pre><code language="language-shell" class="language-shell">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www scheduler-csr.json | cfssljson -bare scheduler
</code></pre>
<ul>
<li>kube-proxy</li>
</ul>
<p>kube-proxy需要跟apiserver进行mTLS认证，生成证书申请请求文件如下</p>
<pre><code language="language-shell" class="language-shell">cat &gt; /etc/kubernetes/pki/proxy-csr.json &lt;&lt;EOF
{
  &#34;CN&#34;: &#34;system:kube-proxy&#34;,
  &#34;key&#34;: {
    &#34;algo&#34;: &#34;rsa&#34;,
    &#34;size&#34;: 2048
  },
  &#34;names&#34;: [
    {
      &#34;C&#34;: &#34;CN&#34;,
      &#34;ST&#34;: &#34;Guangdong&#34;,
      &#34;L&#34;: &#34;Guangzhou&#34;,
      &#34;O&#34;: &#34;kubernetes&#34;,
      &#34;OU&#34;: &#34;system&#34;
    }
  ]
}
EOF
</code></pre>
<p>生成证书</p>
<pre><code language="language-shell" class="language-shell">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www proxy-csr.json | cfssljson -bare proxy
</code></pre>
<ul>
<li>管理员admin能用的证书</li>
</ul>
<p>创建证书信息</p>
<pre><code language="language-shell" class="language-shell">cat &gt; /etc/kubernetes/pki/admin-csr.json &lt;&lt; EOF 
{
  &#34;CN&#34;: &#34;admin&#34;,
  &#34;key&#34;: {
    &#34;algo&#34;: &#34;rsa&#34;,
    &#34;size&#34;: 2048
  },
  &#34;names&#34;: [
    {
      &#34;C&#34;: &#34;CN&#34;,
      &#34;ST&#34;: &#34;Guangzhou&#34;,
      &#34;L&#34;: &#34;Guangdong&#34;,
      &#34;O&#34;: &#34;system:masters&#34;,
      &#34;OU&#34;: &#34;system&#34;
    }
  ]
}
EOF
</code></pre>
<p>生成证书</p>
<pre><code language="language-shell" class="language-shell">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www admin-csr.json | cfssljson -bare admin
</code></pre>
<h2 is-upgraded>3.5 同步证书</h2>
<p>生成证书之后，将证书目录<code>/etc/kubernetes/pki</code>同步到其他主机。</p>


      </google-codelab-step>
    
      <google-codelab-step label="四、安装etcd" duration="0">
        <p>我们将使用<code>k8s-101</code>、<code>k8s-102</code>、<code>k8s-103</code>这三台主机搭建ectd集群。在撰写此文档时（2425.02.18），etcd最新稳定版本是 <code>3.5.18</code>，可以从 <a href="https://github.com/etcd-io/etcd/releases/" target="_blank">https://github.com/etcd-io/etcd/releases/</a> 这个链接下载对应的安装包。</p>
<h2 is-upgraded>4.1 etcd启动参数</h2>
<p>常见参数说明说下</p>
<table>
<tr><td colspan="1" rowspan="1"><p>参数</p>
</td><td colspan="1" rowspan="1"><p>对应环境变量</p>
</td><td colspan="1" rowspan="1"><p>说明</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>–name</p>
</td><td colspan="1" rowspan="1"><p>ETCD_NAME</p>
</td><td colspan="1" rowspan="1"><p>当前etcd的唯一名称，要保证和其他节点不冲突</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>–data-dir</p>
</td><td colspan="1" rowspan="1"><p>ETCD_DATA_DIR</p>
</td><td colspan="1" rowspan="1"><p>指定etcd存储数据的存储位置</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>–listen-peer-urls</p>
</td><td colspan="1" rowspan="1"><p>ETCD_LISTEN_PEER_URLS</p>
</td><td colspan="1" rowspan="1"><p>端对端的通信url，包含主机地址和端口号，指定当前etcd和其他节点etcd通信时的服务地址和端口。</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>–listen-client-urls</p>
</td><td colspan="1" rowspan="1"><p>ETCD_LISTEN_CLIENT_URLS</p>
</td><td colspan="1" rowspan="1"><p>指定当前etcd接收客户端指令的地址和端口，在这里的客户端我们指的是k8s集群的master节点</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>–initial-advertise-peer-urls</p>
</td><td colspan="1" rowspan="1"><p>ETCD_INITIAL_ADVERTISE_PEER_URLS</p>
</td><td colspan="1" rowspan="1"><p>指定etcd广播端口，当前etcd会将数据同步到其他节点，通过2380端口发送</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>–advertise-client-urls</p>
</td><td colspan="1" rowspan="1"><p>ETCD_ADVERTISE_CLIENT_URLS</p>
</td><td colspan="1" rowspan="1"><p>给客户端通告的端口</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>–initial-cluster</p>
</td><td colspan="1" rowspan="1"><p>ETCD_INITIAL_CLUSTER</p>
</td><td colspan="1" rowspan="1"><p>定义etcd集群中所有节点的名称和IP，以及通信端口</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>–initial-cluster-token</p>
</td><td colspan="1" rowspan="1"><p>ETCD_INITIAL_CLUSTER_TOKEN</p>
</td><td colspan="1" rowspan="1"><p>定义etcd中的token，所有节点的token必须保持一致</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>–initial-cluster-state</p>
</td><td colspan="1" rowspan="1"><p>ETCD_INITIAL_CLUSTER_STATE</p>
</td><td colspan="1" rowspan="1"><p>定义etcd集群的状态，new代表新建集群，existing代表加入现有集群</p>
</td></tr>
</table>
<h2 is-upgraded>4.2 创建数据目录</h2>
<p>先创建etcd默认的配置文件目录和数据目录</p>
<pre><code language="language-bash" class="language-bash">mkdir -p /var/lib/etcd/
</code></pre>
<p>安装到<code>/opt</code>目录，后续的k8s集群组件我们将都安装在此</p>
<pre><code language="language-shell" class="language-shell"># 解压
tar -zxvf etcd-v3.5.18-linux-amd64.tar.gz
# 将etc移到/opt目录，并修改etcd目录名
mv etcd-v3.5.18-linux-amd64/ /opt/etcd-v3.5.18
# 创建etcd软链接
ln -s /opt/etcd-v3.5.18 /opt/etcd
</code></pre>
<h2 is-upgraded>4.3 创建etcd启动脚本</h2>
<p>我们先在etcd目录编写启动脚本<code>/opt/etcd/startup.sh</code>，如下</p>
<pre><code language="language-shell" class="language-shell">#!/bin/bash
./etcd \
  --name=&#34;etcd-server-101&#34; \
  --data-dir=&#34;/var/lib/etcd/&#34; \
  --listen-peer-urls=&#34;https://192.168.122.101:2380&#34; \
  --listen-client-urls=&#34;https://192.168.122.101:2379,http://127.0.0.1:2379&#34; \
  --initial-advertise-peer-urls=&#34;https://192.168.122.101:2380&#34; \
  --advertise-client-urls=&#34;https://192.168.122.101:2379&#34; \
  --initial-cluster=&#34;etcd-server-101=https://192.168.122.101:2380,etcd-server-102=https://192.168.122.102:2380,etcd-server-103=https://192.168.122.103:2380&#34; \
  --initial-cluster-token=&#34;etcd-cluster&#34; \
  --initial-cluster-state=&#34;new&#34; \
  --cert-file=&#34;/etc/kubernetes/pki/etcd.pem&#34; \
  --key-file=&#34;/etc/kubernetes/pki/etcd-key.pem&#34; \
  --trusted-ca-file=&#34;/etc/kubernetes/pki/ca.pem&#34; \
  --peer-cert-file=&#34;/etc/kubernetes/pki/etcd.pem&#34; \
  --peer-key-file=&#34;/etc/kubernetes/pki/etcd-key.pem&#34; \
  --peer-trusted-ca-file=&#34;/etc/kubernetes/pki/ca.pem&#34; \
  --peer-client-cert-auth \
  --client-cert-auth
</code></pre>
<p>给启动脚本添加权限</p>
<pre><code language="language-shell" class="language-shell">chmod +x /opt/etcd/startup.sh
</code></pre>
<h2 is-upgraded>4.4 使用supervisor来启动etcd</h2>
<p>现在我们要安装supervisor，用于管理etcd服务，后续的k8s相关组件，我们都用supervisor来管理</p>
<pre><code language="language-shell" class="language-shell"># 安装supervisor
apt install supervisor -y
# 启动supervisor
systemctl start supervisor
# 让superivisor开机自启
systemctl enable supervisor
</code></pre>
<p>添加etcd的supervisor进程维护脚本<code>/etc/supervisor/conf.d/etcd-server.conf</code>，添加以下内容</p>
<pre><code language="language-ini" class="language-ini">[program:etcd-server-101]
directory=/opt/etcd
command=/opt/etcd/startup.sh
numprocs=1
autostart=true
autorestart=true
startsecs=30
startretries=3
exitcodes=0,2
stopsignal=QUIT
stopwaitsecs=10
user=root
redirect_stderr=true
stdout_logfile=/data/logs/supervisor/etcd.stdout.log
stdout_logfile_maxbytes=64MB
stdout_logfile_backups=4
stdout_capture_maxbytes=1MB
stdout_event_enabled=false
</code></pre>
<p>注意：在不同的主机上使用不同的服务名称，这样好辨别，如k8s-101使用<code>etcd-server-101</code>，如k8s-102使用<code>etcd-server-102</code></p>
<p>supervisor相关参数：</p>
<ul>
<li><code>program</code>: 程序名称</li>
<li><code>directory</code>: 脚本目录</li>
<li><code>command</code>: 启动的命令</li>
<li><code>numprocs</code>: 启动的进程数</li>
<li><code>autostart</code>: 是否开启自动启动</li>
<li><code>autorestart</code>: 是否自动重启</li>
<li><code>startsecs</code>: 启动之后多少时间后判定为已起来</li>
<li><code>startretries</code>: 重启次数</li>
<li><code>exitcodes</code>: 退出的code</li>
<li><code>stopsignal</code>: 停止信号</li>
<li><code>stopwaitsecs</code>: 停止等待的时间</li>
<li><code>redirect_stderr</code>: 是否重定向标准输出</li>
<li><code>stdout_logfile</code>: 进程标准输出内容写入文件</li>
<li><code>stdout_logfile_maxbytes</code>: stdout_logfile文件做log滚动时，单个stdout_logfile文件的最大字节数，默认50M，设置为0则认为不做log滚动方式</li>
<li><code>stdout_logfile_backups</code>: stdout_logfile备份文件个数，默认为10</li>
<li><code>stdout_capture_maxbytes</code>: 当进程处于stdout capture mode模式的时候，写入capture FIFO的最大字节数限制，默认为0，此时认为stdout capture mode模式关闭</li>
<li><code>stdout_event_enabled</code>: 如果设置为true，在进程写入标准文件是会发起PROCESS_LOG_STDOUT</li>
</ul>
<p>更新supervisod配置文件</p>
<pre><code language="language-shell" class="language-shell"># 创建supervisor日志目录
mkdir -p /data/logs/supervisor/
# 更新supervisod配置
supervisorctl update
</code></pre>
<p>通过<code>supervisorctl status</code>查询supervisord状态，看到如下内容，代表supervisor正常运行</p>
<pre><code language="language-shell" class="language-shell">root@debian:/opt/etcd# supervisorctl status
etcd-server-101                  RUNNING   pid 85297, uptime 0:04:38
</code></pre>
<p>此时，我们再使用<code>netstat -luntp | grep etcd</code>查看网络服务端口，看到如下信息代表etcd已经正常启动</p>
<pre><code language="language-shell" class="language-shell">root@debian:/opt/etcd# netstat -luntp | grep etcd
tcp        0      0 192.168.122.101:2379      0.0.0.0:*               LISTEN      85298/./etcd        
tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      85298/./etcd        
tcp        0      0 192.168.122.101:2380      0.0.0.0:*               LISTEN      85298/./etcd
</code></pre>
<h2 is-upgraded>4.5 集群验证</h2>
<p>为了方便直接调用<code>etcdctl</code>命令，我们还可以创建其软连接</p>
<pre><code language="language-bash" class="language-bash">ln -s /opt/etcd/etcdctl /usr/local/bin/etcdctl
</code></pre>
<p>我们在任意节点使用etcdctl命令检查集群状态，需要注意的是，要确切指定证书的位置</p>
<pre><code language="language-shell" class="language-shell">etcdctl --cacert=&#34;/etc/kubernetes/pki/ca.pem&#34; --cert=&#34;/etc/kubernetes/pki/etcd.pem&#34; --key=&#34;/etc/kubernetes/pki/etcd-key.pem&#34; --endpoints=&#34;https://192.168.122.101:2379,https://192.168.122.102:2379,https://192.168.122.103:2379&#34; endpoint status --write-out=table
</code></pre>
<p>如果看到如下输出，代表 ectd 集群搭建成功</p>
<pre><code language="language-bash" class="language-bash">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| https://192.168.122.101:2379 | c8815bb4b21730b3 |   3.5.18 |  311 kB |      true |      false |         3 |      37628 |              37628 |        |
| https://192.168.122.102:2379 | f30299e8a0b43b4d |   3.5.18 |  311 kB |     false |      false |         3 |      37628 |              37628 |        |
| https://192.168.122.103:2379 | 61c90f737ccf2682 |   3.5.18 |  311 kB |     false |      false |         3 |      37628 |              37628 |        |
+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
</code></pre>
<p>为了验证etcd集群是否正常工作，我们还可以现在<code>k8s-101</code>设置一个值，如下</p>
<pre><code language="language-bash" class="language-bash">etcdctl put name lixiaoming123
</code></pre>
<p>再通过<code>k8s-102</code>和<code>k8s-103</code>去读取值，如果正常取到，代表etcd集群正常工作，如下命令</p>
<pre><code language="language-bash" class="language-bash">etcdctl get name
</code></pre>
<p>如果需要了解<code>etcdctl</code>这个指令的更多用法，使用<code>--help</code>参数即可查看。</p>


      </google-codelab-step>
    
      <google-codelab-step label="五、将kubernetes二进制安装包解压到系统中" duration="0">
        <p>在撰写这个文档时，kubernetes最新稳定版本为<code>v1.32.2</code>，所以这里也采用这个版本。通过 <a href="https://kubernetes.io/zh-cn/releases/" target="_blank">https://kubernetes.io/zh-cn/releases/</a> 下载最新的对应操作系统的稳定版本。</p>
<p>我们下载好对应的 &#34;Server Binarie&#34; 之后，在所有k8s主机上执行安装，如下步骤</p>
<pre><code language="language-shell" class="language-shell"># 解压安装包
tar -zxvf kubernetes-server-linux-amd64.tar.gz
# 将安装包移到/opt目录下并根据版本重命名
mv kubernetes /opt/kubernetes-v1.32.2
# 创建软连接
ln -s /opt/kubernetes-v1.32.2/ /opt/kubernetes
</code></pre>
<p>在k8s二进制安装目录里包含了k8s源码包，还包含k8s核心组件的docker镜像，因为我们的核心服务不运行在容器里，所以可以删除掉，操作过程如下</p>
<pre><code language="language-shell" class="language-shell"># 进入k8s目录
cd /opt/kubernetes
# 删除源代码
rm kubernetes-src.tar.gz
# 删除二进制文件目录下以tar作为名称后缀的docker镜像包
rm -rf server/bin/*.tar
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="六、安装apiserver" duration="0">
        <p>搭建好etcd数据库集群之后，我们就可以安装apiserver组件了，在所有主机上安装apiserver，以下是具体的安装过程。</p>
<h2 is-upgraded>6.1 创建kubelet授权用户</h2>
<p>因为后面要配置kubelet的bootstrap认证，即kubelet启动时自动创建CSR请求，这里需要在apiserver上开启token的认证。所以先在master上生成一个随机值作为token。下面在一台主机操作即可</p>
<pre><code language="language-shell" class="language-shell"># 创建证书目录
openssl rand -hex 10
</code></pre>
<p>假设生成的token为<code>88c916f382dc619a6bca</code>，把这个token写入到一个文件里，这里写入到 <code>/etc/kubernetes/bb.csv</code>，如下</p>
<pre><code language="language-bash" class="language-bash">cat &gt; /etc/kubernetes/bb.csv &lt;&lt;EOF
88c916f382dc619a6bca,kubelet-bootstrap,10001,&#34;system:node-bootstrapper&#34;
EOF
</code></pre>
<p>这里第二列定义了一个用户名kubelet-bootstrap，后面在配置kubelet时会为此用户授权。创建好该文件后，同步到其他主机。</p>
<h2 is-upgraded>6.2 启动apiser服务</h2>
<h3 is-upgraded>6.2.1 创建启动脚本</h3>
<p>在apiserver二进制文件目录创建<code>/opt/kubernetes/server/bin/kube-apiserver.sh</code>启动脚本文件，写入以下内容</p>
<pre><code language="language-bash" class="language-bash">#!/bin/bash
./kube-apiserver \
    --v=2 \
    --logtostderr=true \
    --allow-privileged=true \
    --bind-address=&#34;192.168.122.101&#34; \
    --secure-port=&#34;6443&#34; \
    --token-auth-file=&#34;/etc/kubernetes/bb.csv&#34; \
    --advertise-address=&#34;192.168.122.101&#34; \
    --service-cluster-ip-range=&#34;10.96.0.0/16&#34; \
    --service-node-port-range=&#34;30000-60000&#34; \
    --etcd-servers=&#34;https://192.168.122.101:2379,https://192.168.122.102:2379,https://192.168.122.103:2379&#34; \
    --etcd-cafile=&#34;/etc/kubernetes/pki/ca.pem&#34; \
    --etcd-certfile=&#34;/etc/kubernetes/pki/etcd.pem&#34; \
    --etcd-keyfile=&#34;/etc/kubernetes/pki/etcd-key.pem&#34; \
    --client-ca-file=&#34;/etc/kubernetes/pki/ca.pem&#34; \
    --tls-cert-file=&#34;/etc/kubernetes/pki/apiserver.pem&#34; \
    --tls-private-key-file=&#34;/etc/kubernetes/pki/apiserver-key.pem&#34; \
    --kubelet-client-certificate=&#34;/etc/kubernetes/pki/apiserver.pem&#34; \
    --kubelet-client-key=&#34;/etc/kubernetes/pki/apiserver-key.pem&#34; \
    --service-account-key-file=&#34;/etc/kubernetes/pki/ca-key.pem&#34; \
    --service-account-signing-key-file=&#34;/etc/kubernetes/pki/ca-key.pem&#34; \
    --service-account-issuer=&#34;https://kubernetes.default.svc.cluster.local&#34; \
    --kubelet-preferred-address-types=&#34;InternalIP,ExternalIP,Hostname&#34; \
    --enable-admission-plugins=&#34;NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota&#34; \
    --authorization-mode=&#34;Node,RBAC&#34; \
    --enable-bootstrap-token-auth=true
    #--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
    #--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
    #--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
    #--requestheader-allowed-names=aggregator  \
    #--requestheader-group-headers=X-Remote-Group  \
    #--requestheader-extra-headers-prefix=X-Remote-Extra-  \
    #--requestheader-username-headers=X-Remote-User
</code></pre>
<p>赋予执行权限</p>
<pre><code language="language-shell" class="language-shell">chmod +x /opt/kubernetes/server/bin/kube-apiserver.sh
</code></pre>
<p>上面注释的部分是配置聚合层的，本环境里没有启用聚合层所以这些选项被注释了，如果配置了聚合层的话，则需要把#取消。相关参数说明</p>
<ul>
<li><code>--v</code>：日志输出级别</li>
<li><code>--logtostderr</code>：将输出记录到标准日志，而不是文件，默认是true</li>
<li><code>--allow-privileged</code>：是否使用超级管理员权限创建容器，默认为false</li>
<li><code>--bind-address</code>：绑定的IP地址，如果没有指定地址（0.0.0.0或者::），默认是 0.0.0.0，代表所有的网卡都在监听服务</li>
<li><code>--secure-port</code>：参数指定的端口号对应监听的IP地址</li>
<li><code>--token-auth-file</code>：该文件用于指定api-server颁发证书的token授权</li>
<li><code>--advertise-address</code>：向集群广播的ip地址，这个ip地址必须能被集群的其他节点访问，如果不指定，将使用–bind-address，如果不指定–bind-addres，将使用默认网卡</li>
<li><code>--service-cluster-ip-range</code>：创建service时，使用的虚拟网段</li>
<li><code>--service-node-port-range</code>：创建service时，服务端口使用的端口范围（默认 30000-32767）</li>
<li><code>--etcd-cafile</code>：访问etcd时使用，ectd的ca文件</li>
<li><code>--etcd-certfile</code>：访问etcd时使用，ectd的证书文件</li>
<li><code>--etcd-servers</code>：各个etcd节点的IP和端口号</li>
<li><code>--etcd-keyfile</code>：访问etcd时使用，ectd的证书私钥文件</li>
<li><code>--client-ca-file</code>：访问apiserver时使用，客户端ca文件</li>
<li><code>--tls-cert-file</code>：访问apiserver时使用，tls证书文件</li>
<li><code>--tls-private-key-file</code>：访问apiserver时使用，tls证书私钥文件</li>
<li><code>--kubelet-client-certificate</code>：访问kubelet时使用，客户端证书路径</li>
<li><code>--kubelet-client-key</code>：访问kubelet时使用，客户端证书私钥</li>
<li><code>--service-account-key-file</code>：包含 PEM 编码的 x509 RSA 或 ECDSA 私钥或公钥，用来检查 ServiceAccount 的令牌</li>
<li><code>--service-account-signing-key-file</code>：指向包含当前服务账号令牌发放者的私钥的文件路径。 此发放者使用此私钥来签署所发放的 ID 令牌</li>
<li><code>--service-account-issuer</code>：服务账户令牌发放者的身份标识</li>
<li><code>--enable-admission-plugins</code>：允许使用的插件</li>
<li><code>--authorization-mode</code>：授权模式</li>
<li><code>--enable-bootstrap-token-auth</code>：是否使用token的方式来自动颁发证书，如果主机节点比较多的时候，手动颁发证书可能不太现实，可以使用基于token的方式自动颁发证书</li>
</ul>
<p>以上是我们在启动apiserver的时候常用的参数，apiserver具有很多参数，很多参数也有默认值，可以<code>./kube-apiserver --hep</code>命令查看更多的帮助。</p>
<h3 is-upgraded>6.2.2 使用supervisor运行</h3>
<p>创建supervisor配置文件<code>/etc/supervisor/conf.d/kube-apiserver.conf</code></p>
<pre><code language="language-ini" class="language-ini">[program:kube-apiserver-160]
directory=/opt/kubernetes/server/bin
command=/opt/kubernetes/server/bin/kube-apiserver.sh
numprocs=1
autostart=true
autorestart=true
startsecs=30
startretries=3
exitcodes=0,2
stopsignal=QUIT
stopwaitsecs=10
user=root
redirect_stderr=true
stdout_logfile=/data/logs/supervisor/apiserver.stdout.log
stdout_logfile_maxbytes=64MB
stdout_logfile_backups=4
stdout_capture_maxbytes=1MB
stdout_event_enabled=false
</code></pre>
<p>更新supervisor服务</p>
<pre><code language="language-shell" class="language-shell">supervisorctl update
</code></pre>
<p>再使用<code>supervisorctl status</code>命令查看apiserver启动状态，如果显示如下内容，代表正常服务</p>
<p>此时，还可以使用<code>netstat -luntp | grep kube-api</code>命令查看网络服务的端口是否正常，如果正常，将返回如下内容</p>


      </google-codelab-step>
    
      <google-codelab-step label="七、搭建L4层负载均衡" duration="0">
        <p>负载均衡是网络层的一种机制，它将请求分发到后端服务器，从而实现高可用和高性能。负载均衡器通常包含一个或多个负载均衡器，每个负载均衡器负责将请求分发到后端服务器。负载均衡器通常使用TCP或UDP协议进行通信，并通过网络层（如TCP或UDP）将请求分发到后端服务器。负载均衡器通常使用轮询、权重、会话保持等功能来优化请求分发。</p>
<p>现在，我们需要在<code>k8s-101</code>和<code>k8s-102</code>上安装nginx作为反向代理服务且两个服务实现负载均衡，再使用keepalived保证高可用性</p>
<h2 is-upgraded>7.1 安装nginx</h2>
<p><code>k8s-101</code>在安装harbor时已经安装过，需要继续在<code>k8s-102</code>上安装</p>
<pre><code language="language-bash" class="language-bash"># 安装依赖
apt install -y gcc make libpcre3-dev libssl-dev zlib1g-dev
# 下载代码
wget https://nginx.org/download/nginx-1.26.3.tar.gz
# 解压文件
tar -zxvf nginx-1.26.3.tar.gz
# 进入源码目录
cd nginx-1.26.3
# 配置编译参数，--prefix参数指定安装目录
./configure \
--prefix=/usr/local/nginx-1.26.3 \
--with-stream \
--with-http_stub_status_module \
--with-http_ssl_module --with-http_v2_module \
--error-log-path=/data/logs/nginx/error.log \
--http-log-path=/data/logs/nginx/access.log
# 编译并安装
make &amp;&amp; make install
# 设置链接
ln -s /usr/local/nginx-1.26.3 /usr/local/nginx
ln -s /usr/local/nginx/sbin/nginx /usr/local/bin/nginx
</code></pre>
<h2 is-upgraded>7.2 配置nginx</h2>
<p>安装完成之后，我们需要两台机的nginx的配置文件<code>/usr/local/nginx/conf/nginx.conf</code>的<code>http</code>节点旁边添加四层反向代码规则，将7443端口的流量使用负载均衡的方式转发到3台主机的6443端口上</p>
<pre><code language="language-shell" class="language-shell"># 设置代理规则
stream {
    upstream kube-apiserver {
        server 192.168.122.101:6443  max_fails=3  fail_timeout=30s;
        server 192.168.122.102:6443  max_fails=3  fail_timeout=30s;
        server 192.168.122.103:6443  max_fails=3  fail_timeout=30s;
    }
    server {
        listen  7443;
        proxy_connect_timeout  2s;
        proxy_timeout  900s;
        proxy_pass kube-apiserver;
    }
}
</code></pre>
<p>在两台主机上配置好规则之后，通过<code>nginx -t</code>命令检查配置结果，如果输出以下内容代表配置正确</p>
<pre><code language="language-shell" class="language-shell">nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
</code></pre>
<p>配置成功之后，启动nginx，如下指令</p>
<pre><code language="language-bash" class="language-bash"># 启动ginx，k8s-101主机使用 nginx -s reload重新加载配置即可
nginx
</code></pre>
<p>要让你手动编译安装的 Nginx 实现开机自启，你可以通过以下几种方式来完成（基于常见的 Linux 系统，如 Ubuntu、CentOS 等）。</p>
<h2 is-upgraded>7.3 使用systemd设置nginx开机自启</h2>
<ul>
<li>创建 Nginx 的 Systemd 服务文件</li>
</ul>
<p>在 <code>/etc/systemd/system/</code> 目录下创建一个 <code>nginx.service</code> 文件：</p>
<pre><code language="language-bash" class="language-bash">vim /etc/systemd/system/nginx.service
</code></pre>
<ul>
<li>添加以下内容到服务文件中</li>
</ul>
<pre><code language="language-ini" class="language-ini">[Unit]
Description=The NGINX HTTP and reverse proxy server
After=network.target

[Service]
ExecStart=/usr/local/nginx/sbin/nginx
ExecReload=/usr/local/nginx/sbin/nginx -s reload
ExecStop=/usr/local/nginx/sbin/nginx -s quit
PIDFile=/usr/local/nginx/logs/nginx.pid
Restart=on-failure
Type=forking

[Install]
WantedBy=multi-user.target
</code></pre>
<ul>
<li>保存并刷新 <code>systemd</code> 配置</li>
</ul>
<pre><code language="language-bash" class="language-bash">systemctl daemon-reload
</code></pre>
<ul>
<li>设置 Nginx 开机自启并立即启动</li>
</ul>
<pre><code language="language-bash" class="language-bash">systemctl enable nginx --now
</code></pre>
<h2 is-upgraded>7.4 安装keepalived</h2>
<p>Keepalived 的虚拟 IP 通过 VRRP 协议在多个服务器间切换，确保服务高可用性和负载均衡。这个虚拟 IP 是 Keepalived 配置的 IP 地址，不属于任何特定服务器，而是由主服务器持有，主服务器故障时切换到备用服务器。我们将使用keepalived实现代理服务器的高可用，以下是安装过程</p>
<pre><code language="language-shell" class="language-shell">apt install keepalived -y
</code></pre>
<p>在两台主机的创建<code>/etc/keepalived/check_port.sh</code>脚本文件，添加以下内容</p>
<pre><code language="language-shell" class="language-shell">#!/bin/bash
CHK_PORT=$1
if [ -n &#34;$CHK_PORT&#34; ]; then
    PORT_PROCESS=`ss -lnt|grep $CHK_PORT|wc -l`
    if [ $PORT_PROCESS -eq 0 ]; then
        echo &#34;Port $CHK_PORT Is Not Used, End&#34;
        exit 1
    fi
else
    echo &#34;Check Port Cant Be Empty!&#34;
    exit 1 
fi
</code></pre>
<p>添加执行权限</p>
<pre><code language="language-shell" class="language-shell">chmod +x /etc/keepalived/check_port.sh
</code></pre>
<p>以上的操作就准备好keepalived的基础环境了，接下来我们使用<code>k8s-101</code>这台主机作为主节点，使用<code>k8s-102</code>作为重节点，进行以下配置</p>
<p><code>k8s-101</code>作为主节点，修改<code>/etc/keepalived/keepalived.conf</code>配置文件如下</p>
<pre><code language="language-shell" class="language-shell">! Configuration File for keepalived
global_defs {
   router_id 192.168.122.101

vrrp_script check_nginx {
    script &#34;/etc/keepalived/check_port.sh 7443&#34;
    interval 2
    weight -20
}

vrrp_instance VI_1 {
    state MASTER
    interface enp1s0
    virtual_router_id 251
    priority 100
    advert_int 1
    mcast_src_ip 192.168.122.101
    nopreempt

    authentication {
        auth_type PASS
        auth_pass 1111
    }

    virtual_ipaddress {
        192.168.122.100
    }
}
</code></pre>
<p><code>k8s-102</code>作为从节点，修改<code>/etc/keepalived/keepalived.conf</code>配置文件如下</p>
<pre><code language="language-shell" class="language-shell">! Configuration File for keepalived

global_defs {
   router_id 192.168.122.102
}

vrrp_script check_nginx {
    script &#34;/etc/keepalived/check_port.sh 7443&#34;
    interval 2
    weight -20
}

vrrp_instance VI_1 {
    state BACKUP
    interface enp1s0
    virtual_router_id 251
    priority 90
    advert_int 1
    mcast_src_ip 192.168.122.101
    nopreempt

    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.122.100
    }
}
</code></pre>
<p>启动服务</p>
<pre><code language="language-shell" class="language-shell"># 重启服务
systemctl restart keepalived
# 设置服务为开机自启
systemctl enable keepalived
</code></pre>
<p>需要注意的是，<code>interface</code>参数对应的是真实的主机网卡名称，<code>virtual_router_id</code>参数需要在同一个虚拟IP的前提下，设置需与主机一个网段的IP。</p>
<h2 is-upgraded>7.5 验证</h2>
<p>通过<code>ping 192.168.122.100</code>的方式进行验证，如果有正常返回，代表keepalived运行正常。</p>
<p>为了验证 Keepalived 的高可用性，可以手动模拟主服务器故障，观察虚拟 IP 是否切换到备用服务器。</p>
<ul>
<li>（1）停止主服务器的 Keepalived 服务</li>
</ul>
<p>在主服务器上执行：</p>
<pre><code language="language-bash" class="language-bash">systemctl stop keepalived
</code></pre>
<ul>
<li>（2）检查备用服务器的虚拟 IP</li>
</ul>
<p>在备用服务器上执行：</p>
<pre><code language="language-bash" class="language-bash">ip addr show
</code></pre>
<p>检查虚拟 IP 是否绑定到备用服务器的网卡。</p>
<ul>
<li>（3）恢复主服务器的 Keepalived 服务</li>
</ul>
<p>在主服务器上执行：</p>
<pre><code language="language-bash" class="language-bash">systemctl start keepalived
</code></pre>
<p>再次检查虚拟 IP 是否切换回主服务器。如果以上操作正常，则说明 Keepalived 的高可用性已经实现，否则需要检查安装过程以及 Keepalived 的配置文件，确保所有参数设置正确。</p>


      </google-codelab-step>
    
      <google-codelab-step label="八、安装controller-manager" duration="0">
        <h2 is-upgraded>8.1 创建kubectl链接</h2>
<p><code>kubectl</code> 是k8s的管理工具，我们创建一个软链接，方便后续使用，如下</p>
<pre><code language="language-shell" class="language-shell">ln -s /opt/kubernetes/server/bin/kubectl /usr/local/bin/kubectl
</code></pre>
<h2 is-upgraded>8.2 安装controller-manager</h2>
<h3 is-upgraded>8.2.1 创建配置</h3>
<p>controller-manager 和 apiserver 之间的认证是通过 kubeconfig 的方式来认证的，即 controller-manager 的私钥、公钥及CA的证书要放在一个 kubeconfig 文件里。下面创建controller-manager所用的kubeconfig文件kube-controller-manager.kubeconfig，现在在<code>/etc/kubernetes/pki</code>里创建，然后移动到<code>/etc/kubernetes</code>里。</p>
<pre><code language="language-bash" class="language-bash"># 进入证书目录
cd /etc/kubernetes/pki/
# 设置集群信息
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.122.100:7443 --kubeconfig=kube-controller-manager.kubeconfig
# 设置用户信息，这里用户名是system:kube-controller-manager ，也就是前面controller-manager-csr.json里CN指定的。
kubectl config set-credentials system:kube-controller-manager --client-certificate=controller-manager.pem --client-key=controller-manager-key.pem --embed-certs=true --kubeconfig=kube-controller-manager.kubeconfig
# 设置上下文信息
kubectl config set-context system:kube-controller-manager --cluster=kubernetes --user=system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig
# 设置默认的上下文
kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig
# 将生成的证书移到/etc/kubernetes/
mv kube-controller-manager.kubeconfig /etc/kubernetes/
</code></pre>
<p><code>/etc/kubernetes/kube-controller-manager.kubeconfig</code>配置文件只需生成一次，再传到其他主机即可。</p>
<h3 is-upgraded>8.2.2 创建启动脚本</h3>
<p>创建文件<code>/opt/kubernetes/server/bin/kube-controller-manager.sh</code>，添加以下内容</p>
<pre><code language="language-shell" class="language-shell">#!/bin/bash
./kube-controller-manager \
    --v=2 \
    --logtostderr=true \
    --bind-address=127.0.0.1 \
    --root-ca-file=/etc/kubernetes/pki/ca.pem \
    --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \
    --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \
    --service-account-private-key-file=/etc/kubernetes/pki/ca-key.pem \
    --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \
    --leader-elect=true \
    --use-service-account-credentials=true \
    --node-monitor-grace-period=40s \
    --node-monitor-period=5s \
    --controllers=*,bootstrapsigner,tokencleaner \
    --allocate-node-cidrs=true \
    --cluster-cidr=10.244.0.0/16 \
    --node-cidr-mask-size=24
</code></pre>
<p>添加执行权限与创建日志目录</p>
<pre><code language="language-shell" class="language-shell"># 添加可执行权限
chmod +x /opt/kubernetes/server/bin/kube-controller-manager.sh
</code></pre>
<p>创建supervisor脚本启动管理文件<code>/etc/supervisor/conf.d/kube-controller-manager.conf</code>，添加以下内容</p>
<pre><code language="language-ini" class="language-ini">[program:kube-controller-manager-101]
directory=/opt/kubernetes/server/bin
command=/opt/kubernetes/server/bin/kube-controller-manager.sh
numprocs=1
autostart=true
autorestart=true
startsecs=30
startretries=3
exitcodes=0,2
stopsignal=QUIT
stopwaitsecs=10
user=root
redirect_stderr=true
stdout_logfile=/data/logs/supervisor/controller.stdout.log
stdout_logfile_maxbytes=64MB
stdout_logfile_backups=4
stdout_capture_maxbytes=1MB
stdout_event_enabled=false
</code></pre>
<p>更新supervisor</p>
<pre><code language="language-shell" class="language-shell">supervisorctl update
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="九、安装scheduler" duration="0">
        <h2 is-upgraded>9.1 创建配置</h2>
<p>scheduler 和 apiserver 之间的认证也是通过 kubeconfig 的方式来认证的，下面创建 scheduler 所用的 kubeconfig 文件 kube-scheduler.kubeconfig，现在在/etc/kubernetes/pki里创建，然后剪切到/etc/kubernetes里。</p>
<pre><code language="language-bash" class="language-bash"># 进入证书目录
cd /etc/kubernetes/pki/
# 设置集群信息
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.122.100:7443 --kubeconfig=kube-scheduler.kubeconfig
# 设置用户信息
kubectl config set-credentials system:kube-scheduler --client-certificate=scheduler.pem --client-key=scheduler-key.pem --embed-certs=true --kubeconfig=kube-scheduler.kubeconfig
# 设置上下文信息
kubectl config set-context system:kube-scheduler --cluster=kubernetes --user=system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
# 设置默认的上下文
kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
# 剪切到/etc/kubernetes
mv kube-scheduler.kubeconfig /etc/kubernetes/
</code></pre>
<p><code>/etc/kubernetes/kube-scheduler.kubeconfig</code>配置文件也只需生成一次，再传到其他主机即可。</p>
<h2 is-upgraded>9.2 创建启动脚本</h2>
<p>创建scheluder启动脚本文件<code>/opt/kubernetes/server/bin/kube-scheduler.sh</code>文件，添加以下内容</p>
<pre><code language="language-shell" class="language-shell">#!/bin/bash
./kube-scheduler \
    --v=2 \
    --bind-address=127.0.0.1 \
    --leader-elect=true \
    --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig
</code></pre>
<p>添加脚本执行权限与创建日志目录</p>
<pre><code language="language-shell" class="language-shell"># 添加脚本的可执行权限
chmod +x /opt/kubernetes/server/bin/kube-scheduler.sh
</code></pre>
<p>创建进程管理配置文件<code>/etc/supervisor/conf.d/kube-scheduler.conf</code>文件，添加以下内容</p>
<pre><code language="language-ini" class="language-ini">[program:kube-scheduler-101]
directory=/opt/kubernetes/server/bin
command=/opt/kubernetes/server/bin/kube-scheduler.sh
numprocs=1
autostart=true
autorestart=true
startsecs=30
startretries=3
exitcodes=0,2
stopsignal=QUIT
stopwaitsecs=10
user=root
redirect_stderr=true
stdout_logfile=/data/logs/supervisor/scheduler.stdout.log
stdout_logfile_maxbytes=64MB
stdout_logfile_backups=4
stdout_capture_maxbytes=1MB
stdout_event_enabled=false
</code></pre>
<p>更新supervisor</p>
<pre><code language="language-shell" class="language-shell">supervisorctl update
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="十、集群验证" duration="0">
        <h2 is-upgraded>10.1 创建管理员配置</h2>
<p>创建管理员用户用的kubeconfig，最后拷贝为 <code>~/.kube/config</code> 作为默认的kubeconfig文件。</p>
<pre><code language="language-bash" class="language-bash"># 进入证书目录
cd /etc/kubernetes/pki/
# 设置一个集群信息
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.122.100:7443 --kubeconfig=admin.conf
# 设置用户信息
kubectl config set-credentials admin --client-certificate=admin.pem --client-key=admin-key.pem --embed-certs=true --kubeconfig=admin.conf
# 设置上下文
kubectl config set-context kubernetes --cluster=kubernetes --user=admin --kubeconfig=admin.conf
# 设置默认上下文境
kubectl config use-context kubernetes --kubeconfig=admin.conf
# 移动
mv admin.conf /etc/kubernetes/
</code></pre>
<p>创建好之后同步到其他节点，再拷贝配置文件到用户目录。</p>
<h2 is-upgraded>10.2 使用管理员配置</h2>
<pre><code language="language-bash" class="language-bash">mkdir -p ~/.kube
cp /etc/kubernetes/admin.conf ~/.kube/config
</code></pre>
<p>使用<code>kubectl get cs</code>检查集群状态，这时候你会发现类似如下的错误</p>
<pre><code language="language-bash" class="language-bash">Error from server (Forbidden): Forbidden (user=admin, verb=get, resource=nodes, subresource=proxy)
</code></pre>
<p>这代表我们创建的<code>admin</code>用户没有集群管理权限，绑定一个<code>cluster-admin</code>角色即可，如下命令</p>
<pre><code language="language-bash" class="language-bash">kubectl create clusterrolebinding system:anonymous --clusterrole=cluster-admin --user=admin
</code></pre>
<p>最后再使用<code>kubectl get cs</code>查看集群，如返回以下类似内容，则代表集群控制节点的服务正常</p>
<pre><code language="language-shell" class="language-shell">root@k8s-101:~# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
scheduler            Healthy   ok        
controller-manager   Healthy   ok        
etcd-0               Healthy   ok
</code></pre>
<p>不过这个命令将来可能会被废弃，目前也可以使用 <code>kubectl cluster-info</code> 命令查看 Kubernetes 集群的基本信息。</p>


      </google-codelab-step>
    
      <google-codelab-step label="十一、安装kubelet" duration="0">
        <h2 is-upgraded>11.1 创建授权配置文件</h2>
<p>为用户 <code>kubelet-bootstrap</code> 授权，允许 <code>kubelet tls bootstrap</code> 创建 <code>CSR</code> 请求，执行如下命令</p>
<pre><code language="language-bash" class="language-bash">kubectl create clusterrolebinding kubelet-bootstrap1 --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
</code></pre>
<p>把 <code>system:certificates.k8s.io:certificatesigningrequests:nodeclient</code> 授权给 <code>kubelet-bootstrap</code>，目的是实现对 <code>CSR</code> 的自动审批，如下命令</p>
<pre><code language="language-bash" class="language-bash">kubectl create clusterrolebinding kubelet-bootstrap2 --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --user=kubelet-bootstrap
</code></pre>
<p>这个用户名是在配置 <code>apiserver</code> 时用到的token文件<code>/etc/kubernetes/bb.csv</code>里指定的。最后使用以下命令创建对应授权配置文件</p>
<pre><code language="language-bash" class="language-bash"># 进入证书目录
cd /etc/kubernetes/pki/
# 创建集群信息
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.122.100:7443 --kubeconfig=kubelet-bootstrap.conf
# 创建用户信息，注意token是上面创建的`bb.csv`里指定的token
kubectl config set-credentials kubelet-bootstrap --token=e83b6b5f1d1dba4cf38a  --kubeconfig=kubelet-bootstrap.conf
# 设置上下文
kubectl config set-context kubernetes --cluster=kubernetes --user=kubelet-bootstrap --kubeconfig=kubelet-bootstrap.conf
# 启用上下文
kubectl config use-context kubernetes --kubeconfig=kubelet-bootstrap.conf
# 剪切配置文件到/etc/kubernetes
mv kubelet-bootstrap.conf  /etc/kubernetes/
</code></pre>
<p>生成配置文件<code>/etc/kubernetes/kubelet-bootstrap.conf</code>之后，传到工作节点中，在这里是<code>k8s-102</code>和<code>k8s-103</code>。</p>
<h2 is-upgraded>11.2 创建kubelet配置文件</h2>
<p>创建 kubelet 用到的配置文件 <code>/etc/kubernetes/kubelet-config.yaml</code>，后续 kubelet 配置启动文件需要用到，内容如下</p>
<pre><code language="language-yaml" class="language-yaml">apiVersion: kubelet.config.k8s.io/v1beta1
address: 0.0.0.0
port: 10250 
readOnlyPort: 10255
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.pem
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
httpCheckFrequency: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
</code></pre>
<p>这里我们指定clusterDNS的IP是<code>10.96.0.10</code>，后续我们会在<code>kube-dns</code>中配置<code>CoreDNS</code>的IP为<code>10.96.0.10</code>。</p>
<h2 is-upgraded>11.3 配置kubelet启动脚本</h2>
<h3 is-upgraded>11.3.1 配置启动脚本</h3>
<p>接下来在<code>k8s-102</code>和<code>k8s-103</code>上启动kubelet，在让kubelet启动之前，我们需要有一个基础的pause镜像，以下是拉取命令，该镜像负责其k8s集群中pod启动之前的初始化操作</p>
<pre><code language="language-bash" class="language-bash">nerdctl pull registry.aliyuncs.com/google_containers/pause:3.10
</code></pre>
<p>创建kubelet的启动脚本文件<code>/opt/kubernetes/server/bin/kubelet.sh</code>文件，添加以下内容</p>
<pre><code language="language-bash" class="language-bash">#!/bin/bash
./kubelet \
    --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.conf  \
    --cert-dir=/var/lib/kubelet/pki \
    --hostname-override=node-102 \
    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
    --config=/etc/kubernetes/kubelet-config.yaml \
    --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.10 \
    --container-runtime=remote \
    --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \
    --runtime-request-timeout=15m \
    --v=2
</code></pre>
<p>添加可执行权限</p>
<pre><code language="language-bash" class="language-bash">chmod +x /opt/kubernetes/server/bin/kubelet.sh
</code></pre>
<p>创建数据目录和日志目录</p>
<pre><code language="language-bash" class="language-bash"># 创建kubelet所需要的日志目录
mkdir -p /var/log/kubernetes
</code></pre>
<h3 is-upgraded>11.3.2 配置管理服务</h3>
<p>创建supervisor进程配置文件<code>/etc/supervisor/conf.d/kube-kubelet.conf</code>文件，添加以下内容</p>
<pre><code language="language-ini" class="language-ini">[program:kube-kubelet-102]
directory=/opt/kubernetes/server/bin
command=/opt/kubernetes/server/bin/kubelet.sh
numprocs=1
autostart=true
autorestart=true
startsecs=30
startretries=3
exitcodes=0,2
stopsignal=QUIT
stopwaitsecs=10
user=root
redirect_stderr=true
stdout_logfile=/data/logs/supervisor/kubelet.stdout.log
stdout_logfile_maxbytes=64MB
stdout_logfile_backups=4
stdout_capture_maxbytes=1MB
stdout_event_enabled=false
</code></pre>
<p>更新supervisord，如下命令</p>
<pre><code language="language-shell" class="language-shell">supervisorctl update
</code></pre>
<h3 is-upgraded>11.3.3 验证集群</h3>
<p>此时，服务已经正常运行了，可以使用以下<code>kubectl</code>命令在查看节点信息</p>
<pre><code language="language-shell" class="language-shell">kubectl get nodes
</code></pre>
<p>如果看到以下信息，代表安装成功</p>
<pre><code language="language-bash" class="language-bash">NAME       STATUS   ROLES    AGE   VERSION
node-102   Ready    &lt;none&gt;   19m   v1.32.2
node-103   Ready    &lt;none&gt;   18m   v1.32.2
</code></pre>
<p>我们还可以设置集群的标签</p>
<pre><code language="language-bash" class="language-bash"># 设置集群为node标签
kubectl label node node-102 node-role.kubernetes.io/node=
kubectl label node node-103 node-role.kubernetes.io/node=
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="十二、安装proxy" duration="0">
        <h2 is-upgraded>12.1 创建kubeconfig配置文件</h2>
<p>在<code>k8s-101</code>服务器上执行，如下命令</p>
<pre><code language="language-bash" class="language-bash"># 进入证书目录
cd /etc/kubernetes/pki/
# 创建集群信息
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.122.100:7443 --kubeconfig=kube-proxy.kubeconfig
# 创建用户信息
kubectl config set-credentials kube-proxy --client-certificate=proxy.pem --client-key=proxy-key.pem --embed-certs=true --kubeconfig=kube-proxy.kubeconfig
# 创建上下文
kubectl config set-context default --cluster=kubernetes --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig
# 应用上下文
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
# 移动到/etc/kubernetes/
mv kube-proxy.kubeconfig /etc/kubernetes/
</code></pre>
<p>创建完成后，同步到工作节点<code>192-debian</code>和<code>160-debian</code>。</p>
<h2 is-upgraded>12.2 创建kube-proxy配置文件</h2>
<p>在工作节点创建<code>/etc/kubernetes/kube-proxy.yaml</code>，内容如下</p>
<pre><code language="language-bash" class="language-bash">apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
clientConnection:
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
clusterCIDR: 10.244.0.0/16 
kind: KubeProxyConfiguration
metricsBindAddress: 0.0.0.0:10249
mode: &#34;ipvs&#34;
</code></pre>
<h2 is-upgraded>12.3 创建启动脚本</h2>
<p>在两台主机执行以上脚本之后，我们创建<code>kube-proxy</code>的启动脚本文件<code>/opt/kubernetes/server/bin/kube-proxy.sh</code></p>
<pre><code language="language-shell" class="language-shell">#!/bin/bash
./kube-proxy \
  --config=/etc/kubernetes/kube-proxy.yaml \
  --v=2
</code></pre>
<p>添加可执行权限</p>
<pre><code language="language-shell" class="language-shell">chmod +x /opt/kubernetes/server/bin/kube-proxy.sh
</code></pre>
<h2 is-upgraded>13.4 创建服务配置</h2>
<p>创建supervisor的配置文件<code>/etc/supervisor/conf.d/kube-proxy.conf</code>文件，添加以下内容</p>
<pre><code language="language-ini" class="language-ini">[program:kube-proxy-102]
directory=/opt/kubernetes/server/bin
command=/opt/kubernetes/server/bin/kube-proxy.sh
numprocs=1
autostart=true
autorestart=true
startsecs=30
startretries=3
exitcodes=0,2
stopsignal=QUIT
stopwaitsecs=10
user=root
redirect_stderr=true
stdout_logfile=/data/logs/supervisor/kube-proxy.stdout.log
stdout_logfile_maxbytes=64MB
stdout_logfile_backups=4
stdout_capture_maxbytes=1MB
stdout_event_enabled=false
</code></pre>
<p>更新supervisor</p>
<pre><code language="language-shell" class="language-shell">supervisorctl update
</code></pre>
<h2 is-upgraded>12.5 创建权限配置文件</h2>
<p>在<code>k8s-101</code>上创建<code>/etc/kubernetes/rbac.yaml</code>，写入如下内容</p>
<pre><code language="language-yml" class="language-yml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &#34;true&#34;
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kubernetes-to-kubelet
rules:
  - apiGroups:
      - &#34;&#34;
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - &#34;*&#34;
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kubernetes
  namespace: &#34;&#34;
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kubernetes-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
</code></pre>
<h2 is-upgraded>12.6 创建角色及授权</h2>
<pre><code language="language-bash" class="language-bash">kubectl apply -f /etc/kubernetes/rbac.yaml
</code></pre>
<h2 is-upgraded>12.7 集群的验证</h2>
<p>在两个节点都启动好kube-proxy服务之后，至此，集群的基本组件已经安装完成，下面我们来验证集群。在任意管理节点创建一个Pod类型的资源，添加<code>nginx-pod.yml</code>文件，添加以下内容</p>
<pre><code language="language-yaml" class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: nginx-pod
    image: nginx:alpine
    imagePullPolicy: IfNotPresent
    ports:
    - name: nginxport
      containerPort: 80
</code></pre>
<p>执行资源创建命令</p>
<pre><code language="language-shell" class="language-shell">kubectl create -f nginx-pod.yml
</code></pre>
<p>使用以下命令验证pod是否正常运行</p>
<pre><code language="language-shell" class="language-shell">kubectl get pod -o wide
</code></pre>
<p>如果返回如下内容，代表集群正常</p>
<pre><code language="language-shell" class="language-shell">NAME   READY   STATUS    RESTARTS   AGE   IP          NODE       NOMINATED NODE   READINESS GATES
pod1   1/1     Running   0          94s   10.22.0.2   node-103   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>创建的pod运行在<code>node-103</code>这台主机上，在这台机使用<code>curl 10.22.0.2</code>命令能正常访问到nginx服务。但是如果我们在另一个节点<code>node-102</code>上执行<code>curl 10.22.0.2</code>会发现访问不到。原因是这两个节点上的容器在各自的虚拟网络内，我们将到后续的章节安装通过安装 k8s 网络插件的方式，实现不同工作节点的容器网络互相访问的功能。</p>


      </google-codelab-step>
    
      <google-codelab-step label="十三、安装网络插件" duration="0">
        <p>以下的操作，我们在<code>k8s-101</code>节点去完成。</p>
<h2 is-upgraded>13.1 安装calico</h2>
<p>Calico 是 Kubernetes 集群的网络基础设施，负责 Pod 的网络连接、跨节点通信和网络策略管理。根据以下步骤进行安装。</p>
<pre><code language="language-bash" class="language-bash">cd /etc/kubernetes
wget https://docs.projectcalico.org/manifests/calico.yaml
</code></pre>
<p>修改calico.yaml文件，将<code>CALICO_IPV4POOL_CIDR</code>改为和<code>kube-proxy</code>的配置一样，如下</p>
<pre><code language="language-ini" class="language-ini">- name: CALICO_IPV4POOL_CIDR
  value: &#34;10.244.0.0/16&#34;
</code></pre>
<p>在calico配置文件中，定义了一下容器镜像，在运行calico的时候将会用到，可以使用 <code>cat calico.yaml  | grep image</code> 命令查看所有需要的镜像列表，如下</p>
<pre><code language="language-yaml" class="language-yaml">image: docker.io/calico/cni:v3.25.0
imagePullPolicy: IfNotPresent
image: docker.io/calico/cni:v3.25.0
imagePullPolicy: IfNotPresent
image: docker.io/calico/node:v3.25.0
imagePullPolicy: IfNotPresent
image: docker.io/calico/node:v3.25.0
imagePullPolicy: IfNotPresent
image: docker.io/calico/kube-controllers:v3.25.0
imagePullPolicy: IfNotPresent
</code></pre>
<p>我们可以看到，这些镜像都来自<code>docker.io</code>，但因为一些原因，在撰写这篇文档时，国内访问 <code>docker.io</code> 的网络不太顺畅。因此你需要想办法让你的工作节点宿主机能拉取到这些镜像，最后再创建<code>calico</code>服务。或者你需要修改配置文件，改成这些镜像在可以拉取到的国内镜像站对应的镜像名。</p>
<p>解决依赖的镜像的拉取问题后，最后创建calico服务</p>
<pre><code language="language-bash" class="language-bash">kubectl apply -fcalico.yaml
</code></pre>
<p>执行命令之后，calico会拉去远端的镜像并运行，执行 <code>kubectl get pods -n kube-system</code> 等到所有pod都处于<code>Running</code>状态代表服务启动完成，如下</p>
<pre><code language="language-bash" class="language-bash">NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6799f5f4b4-xqpf9   1/1     Running   0          3m34s
calico-node-9bt29                          1/1     Running   0          3m34s
calico-node-djxvc                          1/1     Running   0          3m34s
</code></pre>
<p>此时calico已经正常运行了，如果上节创建的nginx的pod还没有删除的话，先删除掉再创建，如下命令</p>
<pre><code language="language-bash" class="language-bash">kubectl delete -f nginx-pod.yaml
kubectl apply -f nginx-pod.yaml
</code></pre>
<p>创建新的pod之后，使用<code>kubectl get pod -o wide</code>查看pod所处的节点，此时我们在任意工作节点请求该IP，都能成功请求。</p>
<h2 is-upgraded>13.2 安装coredns</h2>
<h3 is-upgraded>13.2.1 下载基础资源配置文件</h3>
<p>下载corndns资源配置文件</p>
<pre><code language="language-bash" class="language-bash"># 下载
wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed
# 重命名
mv coredns.yaml.sed coredns.yaml
</code></pre>
<h3 is-upgraded>13.2.2 修改配置</h3>
<p>做出以下修改：</p>
<p>大概第62行，找到配置文件中的<code>CLUSTER_DOMAIN</code>、<code>REVERSE_CIDRS</code>这两个变量改为集群域名，如下</p>
<pre><code language="language-yml" class="language-yml">kubernetes cluster.local in-addr.arpa ip6.arpa {
  fallthrough in-addr.arpa ip6.arpa
}
</code></pre>
<p>大概第66行，<code>UPSTREAMNAMESERVER</code>改为宿主机DNS配置<code>/etc/resolve.conf</code>，如下</p>
<pre><code language="language-yaml" class="language-yaml">forward . /etc/resolve.conf {
  max_concurrent 1000
}
</code></pre>
<p>大概在186行，将<code>CLUSTER_DNS_IP</code>改为kubelet配置文件中指定的集群IP地址<code>10.96.0.10</code>，如下</p>
<pre><code language="language-yml" class="language-yml">spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.96.0.10
</code></pre>
<h3 is-upgraded>13.2.3 启动服务</h3>
<p>使用以下命令启动服务</p>
<pre><code language="language-bash" class="language-bash">kubectl apply -f coredns.yaml
</code></pre>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/claat-public/native-shim.js"></script>
  <script src="https://storage.googleapis.com/claat-public/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/claat-public/prettify.js"></script>
  <script src="https://storage.googleapis.com/claat-public/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
